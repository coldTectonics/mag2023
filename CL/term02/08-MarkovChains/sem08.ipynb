{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ae5533-bc8c-4dbc-b09a-b8c98053e687",
   "metadata": {},
   "source": [
    "#### Часть 1.\n",
    "\n",
    "Собственная имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99350a4-ed41-468c-b934-b1754883f3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6583e-51f0-47c1-9d4c-be99d3f168f8",
   "metadata": {},
   "source": [
    "Прочитаем текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a37f50-ea52-4685-8a10-564ca2e76a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('zbch.txt') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5c383-4b78-421c-8895-c923a0d4e429",
   "metadata": {},
   "source": [
    "Разделим на слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc5d9f-b7d4-43c8-ae9f-b5742090d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1da92-90ee-4ac4-9475-31de086c13b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "ind_words = [t.text for t in tokenize(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3eafd6-4f06-4cbb-8eb2-e91a0bd660fe",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет нам собирать наши пары слов, и с ее помощью ниже получим словарь с переходами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15a605-ecf1-40c4-81b8-844e177a30d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_pairs(ind_words):\n",
    "    for i in range(len(ind_words) - 1):\n",
    "        yield ind_words[i], ind_words[i + 1]\n",
    "        \n",
    "pair = make_pairs(ind_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db42b98-63c4-4edc-8ecc-511c43e957ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for word_1, word_2 in pair:\n",
    "    if word_1 in word_dict.keys():\n",
    "        word_dict[word_1].append(word_2)\n",
    "    else:\n",
    "        word_dict[word_1] = [word_2]\n",
    "        \n",
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac8615-a47a-40d9-b896-91dd712acfc0",
   "metadata": {},
   "source": [
    "Рандомно выберем первое слово и начнем генерировать текст длиной в 20 слов, случайно выбирая возможные следующие токены по предыдущему. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8c7f0-a697-4b7b-b37c-0a95bbd3b502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_word = np.random.choice(ind_words)\n",
    "chain = [first_word]\n",
    "n_words = 20\n",
    "\n",
    "for i in range(n_words):\n",
    "    chain.append(np.random.choice(word_dict[chain[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511945e-fbe1-4c94-bfb4-08a48d7a56b3",
   "metadata": {},
   "source": [
    "Наш обучающий текст был очень маленьким, поэтому, скорее всего, сгенерируются его же фрагменты вперемешку. На больших текстах результат будет интереснее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb5a45-4894-4568-adca-e9c3a5459050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27af5c0-acb8-489f-93bc-9b323334805f",
   "metadata": {},
   "source": [
    "#### Часть 2\n",
    "\n",
    "Теперь будем использовать готовую библиотеку markovify. Для предобработки текстов (возьмем Шекспира) нам понадобятся nltk и spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664104d-155b-45e3-bb4c-a409ba50321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install markovify\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b42fd6-328a-4472-8885-3905d76ffa1e",
   "metadata": {},
   "source": [
    "Загрузим гутенберговский корпус из nltk и возьмем три пьесы Шекспира из него:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969c8f4-b4fb-4e29-a49f-e213fcf60b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import markovify\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15862d-2b07-42a0-87e5-b2421fe01f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "# выведем по 100 первых символов\n",
    "print('\\nRaw:\\n', hamlet[:100])\n",
    "print('\\nRaw:\\n', macbeth[:100])\n",
    "print('\\nRaw:\\n', caesar[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96c77f-0410-495f-a30e-93523ae7b3b8",
   "metadata": {},
   "source": [
    "Напишем всякие функции предобработки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17350789-b8cb-4abf-9cc2-12adc33134ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# поудаляем ненужные пробелы, отступы, пунктуацию и прочее\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub('[\\[].*?[\\]]', '', text)\n",
    "    text = re.sub(r'(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b','', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bf493-a088-469b-9278-74ccc3e64bb3",
   "metadata": {
    "id": "xJIKvUdtCBiT"
   },
   "outputs": [],
   "source": [
    "# удалим заголовки глав\n",
    "hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
    "macbeth = re.sub(r'Chapter \\d+', '', macbeth)\n",
    "caesar = re.sub(r'Chapter \\d+', '', caesar)\n",
    "# применим функции к текстам\n",
    "hamlet = text_cleaner(hamlet)\n",
    "caesar = text_cleaner(caesar)\n",
    "macbeth = text_cleaner(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444483e5-e93e-440b-bb47-33c0cc59553f",
   "metadata": {
    "id": "ePqTm8Ux5G0m"
   },
   "outputs": [],
   "source": [
    "# распарсим с помощью спейси: это даст нам токены, предложения и потом части речи\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "hamlet_doc = nlp(hamlet)\n",
    "macbeth_doc = nlp(macbeth)\n",
    "caesar_doc = nlp(caesar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16b6ca-4f13-442e-9a39-2278caddb29b",
   "metadata": {},
   "source": [
    "Соберем предложения, отсеяв однословные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870bf494-70fa-4e57-b176-85d62647ffd9",
   "metadata": {
    "id": "65ubgvgSVd4M"
   },
   "outputs": [],
   "source": [
    "hamlet_sents = ' '.join([sent.text for sent in hamlet_doc.sents if len(sent.text) > 1])\n",
    "macbeth_sents = ' '.join([sent.text for sent in macbeth_doc.sents if len(sent.text) > 1])\n",
    "caesar_sents = ' '.join([sent.text for sent in caesar_doc.sents if len(sent.text) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ad057-2496-4773-af7f-7c19e0f6a6b4",
   "metadata": {
    "id": "uecwplqXU_Hh"
   },
   "outputs": [],
   "source": [
    "shakespeare_sents = hamlet_sents + macbeth_sents + caesar_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7eb48-c9ea-4e45-a18e-56d113d1b16e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5Z-g9YWkxdC",
    "outputId": "e3b66f63-d225-447a-e491-6cb518267e3b"
   },
   "outputs": [],
   "source": [
    "print(shakespeare_sents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae6246-b3be-484c-8aa5-01c75701eb29",
   "metadata": {},
   "source": [
    "Теперь самое интересное: инициализируем наш markovify предложениями из текстов. Внутри этот объект уже сам сделает все, что нам надо, и нам останется только генерировать \"предложения\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891c7ad-d860-4a53-ac67-a24e47afbd9f",
   "metadata": {
    "id": "xDTCPyvpQyK9"
   },
   "outputs": [],
   "source": [
    "generator_1 = markovify.Text(shakespeare_sents, state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24299f-39df-4b97-bbdf-e21db5571fb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMdKoot4QyHU",
    "outputId": "7432af31-510a-40d1-8fac-c6ad2c65024a"
   },
   "outputs": [],
   "source": [
    "# попробуем сгенерировать три предложения без всяких ограничений\n",
    "for i in range(3):\n",
    "    print(generator_1.make_sentence())\n",
    "\n",
    "# сгенерируем предложения с ограничением: не больше 100 символов\n",
    "for i in range(3):\n",
    "    print(generator_1.make_short_sentence(max_chars=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb9207-f91d-458f-a395-fd807f9cfc08",
   "metadata": {},
   "source": [
    "Прикольно, но не совсем. Давайте для улучшения нашего генератора используем информацию о частях речи: мы ее просто приклеим к нашим токенам, переопределив пару методов в классе Text, так, чтобы при разбиении предложений в исходных данных он приклеивал части речи и использовал их, собирая матрицу переходов, а при создании генерированных предложений обратно отклеивал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228bdfa-2f47-4f71-b8f8-d9ae4e44cb0e",
   "metadata": {
    "id": "R_KFS5-YWZ3X"
   },
   "outputs": [],
   "source": [
    "class POSifiedText(markovify.Text):\n",
    "\n",
    "    def word_split(self, sentence):\n",
    "        return ['::'.join((word.orth_, word.pos_)) for word in nlp(sentence)]\n",
    "\n",
    "    def word_join(self, words):\n",
    "        sentence = ' '.join(word.split('::')[0] for word in words)\n",
    "        return sentence\n",
    "\n",
    "# инициализируем экземпляр доопределенного класса\n",
    "generator_2 = POSifiedText(shakespeare_sents, state_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be34dab-b46a-4843-8b4b-3b09d4db98fd",
   "metadata": {},
   "source": [
    "А теперь потестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420c03f-b464-4f1a-bd62-ecdc82d00e7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcdtchcUYS3W",
    "outputId": "389dd5d5-c2b2-4ed0-c342-0408c219dc74"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generator_2.make_sentence())\n",
    "\n",
    "for i in range(5):\n",
    "    print(generator_2.make_short_sentence(max_chars=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc73b2-cd8b-4839-b5f4-a1d2038b0a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
